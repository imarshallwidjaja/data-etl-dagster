# =============================================================================
# Dagster User Code Dockerfile (with GDAL)
# =============================================================================
# This image contains the actual ETL pipeline code and GDAL libraries.
# It runs as a gRPC server that the Dagster daemon connects to.
# =============================================================================

FROM ghcr.io/osgeo/gdal:ubuntu-full-3.8.0

# Install Python and system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    python3-venv \
    libpq-dev \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Create and activate virtual environment
ENV VIRTUAL_ENV=/opt/venv
RUN python3 -m venv $VIRTUAL_ENV
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Set working directory
WORKDIR /opt/dagster/app

# Install Python dependencies
COPY services/dagster/requirements-user-code.txt .
RUN pip install --no-cache-dir -r requirements-user-code.txt

# Pre-install DuckDB extensions for offline/air-gapped operation.
# Extensions are cached in ~/.duckdb/extensions and only LOAD is needed at runtime.
RUN python3 -c "import duckdb; con = duckdb.connect(); con.execute('INSTALL httpfs'); con.execute('INSTALL spatial'); con.close()"

# Pre-download NLTK stopwords for offline operation (used by tabular_headers).
RUN python3 -c "import nltk; nltk.download('stopwords', quiet=True)"

# Copy and install the libs package
COPY libs /opt/dagster/app/libs
RUN pip install --no-cache-dir /opt/dagster/app/libs

# Set GDAL environment variables
ENV GDAL_DATA=/usr/share/gdal
ENV PROJ_LIB=/usr/share/proj

# Copy the ETL pipeline code (for production builds)
# In development, this is overridden by the volume mount in docker-compose.yaml
COPY services/dagster/etl_pipelines /opt/dagster/app/etl_pipelines

# Expose gRPC port
EXPOSE 4000

# Default command (overridden in docker-compose)
CMD ["dagster", "api", "grpc", "-h", "0.0.0.0", "-p", "4000", "-m", "etl_pipelines"]

